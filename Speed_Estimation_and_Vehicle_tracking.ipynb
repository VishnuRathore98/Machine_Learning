{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkQoBIwWeNoDBidj2wzq5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuRathore98/Machine_Learning/blob/master/Speed_Estimation_and_Vehicle_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btO5G2vnx4Jd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision inference tqdm opencv-python numpy collection typing ultralytics"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RM0MB8tgyMWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "# For computations\n",
        "import numpy as np\n",
        "\n",
        "# For computer vision tasks\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# For getting the model\n",
        "import supervision as sv\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "# For displaying video\n",
        "from IPython.display import Video\n",
        "\n",
        "# For getting the model for detection\n",
        "from inference.models.utils import get_roboflow_model\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For gpu acceleration\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "682GXcvI246P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if cuda is available\n",
        "torch.cuda.is_available()\n",
        "# Select cuda as device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sVQQIy0_nRW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the video\n",
        "download_assets(VideoAssets.VEHICLES)"
      ],
      "metadata": {
        "id": "K55dwB4_DxRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring variables\n",
        "SOURCE_VIDEO = \"vehicles.mp4\"\n",
        "TARGET_VIDEO = \"vehicles-result.mp4\"\n",
        "SOURCE = np.array([[1252, 787], [2298, 803], [5039, 2159], [-550, 2159]])\n",
        "TARGET_WIDTH = 25\n",
        "TARGET_HEIGHT = 250\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH-1, 0],\n",
        "    [TARGET_WIDTH-1, TARGET_HEIGHT-1],\n",
        "    [0, TARGET_HEIGHT-1]\n",
        "])"
      ],
      "metadata": {
        "id": "0wtK-XUuGR_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Playing the video\n",
        "# display(Video(SOURCE_VIDEO, embed=True))"
      ],
      "metadata": {
        "id": "UB5LVipOD35c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For resolving perspective distortion and get the coordinates for objects as per their position in the frame\n",
        "class ViewTransformer:\n",
        "  def __init__(self, source, target):\n",
        "    source = source.astype(np.float32)\n",
        "    target = target.astype(np.float32)\n",
        "    self.matrix = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "  def transformed_points(self, points):\n",
        "    reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "    transformed_points = cv2.perspectiveTransform(reshaped_points, self.matrix)\n",
        "    return transformed_points.reshape(-1, 2)"
      ],
      "metadata": {
        "id": "3We5d2S6obDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from typing import DefaultDict\n",
        "# Getting information about the video\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n",
        "\n",
        "# Get the model from roboflow\n",
        "model = get_roboflow_model(\"yolov11x-seg-640\")\n",
        "\n",
        "# Using byte track to track individual object to get its id using video frames\n",
        "byte_track = sv.ByteTrack(frame_rate=video_info.fps)\n",
        "\n",
        "# Getting bounding box line and text thickness\n",
        "thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\n",
        "text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "# Bounding boxes\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness, color_lookup=sv.ColorLookup.TRACK)\n",
        "\n",
        "# Labelling the bounding box\n",
        "label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness, text_position=sv.Position.BOTTOM_CENTER, color_lookup=sv.ColorLookup.TRACK)\n",
        "\n",
        "# For tracing moving vehicle's track\n",
        "trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=video_info.fps*2, position=sv.Position.BOTTOM_CENTER, color_lookup=sv.ColorLookup.TRACK)\n",
        "\n",
        "# Plotting the polygon box to limit the detection boundary, and calculating speed\n",
        "polygon_zone = sv.PolygonZone(SOURCE, frame_resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "\n",
        "\n",
        "# Calling the ViewTransformer\n",
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
        "\n",
        "# Getting video frames\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO)\n",
        "\n",
        "# Getting the coordinate history\n",
        "coordinates = DefaultDict(lambda: deque(maxlen=video_info.fps))"
      ],
      "metadata": {
        "id": "o7qvrHGaE5KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # ----------------------------------------------------------------------------------------------------------------------\n",
        "# # Annotating a single frame\n",
        "# frame = iter(frame_generator)\n",
        "# frame = next(frame)\n",
        "\n",
        "# result = model.infer(frame)[0]\n",
        "# detections = sv.Detections.from_inference(result)\n",
        "\n",
        "# # Detecting only inside polygon zone\n",
        "# detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "# # Labelling objects with id's\n",
        "# detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "# #\n",
        "# points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "# points = view_transformer.transformed_points(points).astype(int)\n",
        "\n",
        "# # Labels list\n",
        "# labels = []\n",
        "\n",
        "# for tracker_id, [_, y] in zip(detections.tracker_id, points):\n",
        "#   coordinates[tracker_id].append(y)\n",
        "#   if len(coordinates[tracker_id]) < video_info.fps/2:\n",
        "#     labels.append(f\"#{tracker_id}\")\n",
        "#   else:\n",
        "#     coordinate_start = coordinates[tracker_id][-1]\n",
        "#     coordinate_end = coordinates[tracker_id][0]\n",
        "#     distance = abs(coordinate_start - coordinate_end)\n",
        "#     time = len(coordinates[tracker_id]) / video_info.fps\n",
        "#     speed = distance / time * 3.6\n",
        "#     labels.append(f\"#{tracker_id} {int(speed)}km/h\")\n",
        "\n",
        "# # Annotating the frame\n",
        "\n",
        "# annotated_frame = frame.copy()\n",
        "\n",
        "# # Drawing the polygon onto the frame\n",
        "# annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.RED)\n",
        "\n",
        "# annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "# annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "# sv.plot_image(annotated_frame)\n",
        "# # ---------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "xRKQRYUAGXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# open target video\n",
        "with sv.VideoSink(TARGET_VIDEO, video_info) as sink:\n",
        "    # Looping over frames to annotate vehicles frame by frame\n",
        "    # loop over source video frame\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "\n",
        "      result = model.infer(frame)[0]\n",
        "\n",
        "      detections = sv.Detections.from_inference(result)\n",
        "\n",
        "      # Detecting only inside polygon zone\n",
        "      detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "      # Labelling objects with id's\n",
        "      detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "      #\n",
        "      points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "      points = view_transformer.transformed_points(points).astype(int)\n",
        "\n",
        "      # Labels list\n",
        "      labels = []\n",
        "\n",
        "      for tracker_id, [_, y] in zip(detections.tracker_id, points):\n",
        "        coordinates[tracker_id].append(y)\n",
        "        if len(coordinates[tracker_id]) < video_info.fps/2:\n",
        "          labels.append(f\"#{tracker_id}\")\n",
        "        else:\n",
        "          coordinate_start = coordinates[tracker_id][-1]\n",
        "          coordinate_end = coordinates[tracker_id][0]\n",
        "          distance = abs(coordinate_start - coordinate_end)\n",
        "          time = len(coordinates[tracker_id]) / video_info.fps\n",
        "          speed = distance / time * 3.6\n",
        "          labels.append(f\"#{tracker_id} {int(speed)}km/h\")\n",
        "\n",
        "      # Annotating the frame\n",
        "\n",
        "      annotated_frame = frame.copy()\n",
        "\n",
        "      # # Drawing the polygon onto the frame\n",
        "      # annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.RED)\n",
        "\n",
        "      annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "\n",
        "      annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "      annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "      # Writing the annotated frame to the target video\n",
        "      sink.write_frame(annotated_frame)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F5fg1f2EH5PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkCgvhBPJNRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}