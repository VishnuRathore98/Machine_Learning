{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyON93sGkvjR8RLEmthR8iWt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuRathore98/Machine_Learning/blob/master/Speed_Estimation_and_Vehicle_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btO5G2vnx4Jd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision inference ultralytics"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RM0MB8tgyMWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "# For computations\n",
        "import numpy as np\n",
        "\n",
        "# For computer vision tasks\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# For getting the model\n",
        "import supervision as sv\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "# For displaying video\n",
        "from IPython.display import Video\n",
        "\n",
        "# For getting the model for detection\n",
        "from inference.models.utils import get_roboflow_model"
      ],
      "metadata": {
        "id": "682GXcvI246P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the video\n",
        "download_assets(VideoAssets.VEHICLES)"
      ],
      "metadata": {
        "id": "K55dwB4_DxRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring variables\n",
        "SOURCE_VIDEO = \"vehicles.mp4\"\n",
        "TARGET_VIDEO = \"vehicles-result.mp4\"\n",
        "SOURCE = np.array([[1252, 787], [2298, 803], [5039, 2159], [-550, 2159]])\n",
        "TARGET_WIDTH = 25\n",
        "TARGET_HEIGHT = 250\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH-1, 0],\n",
        "    [TARGET_WIDTH-1, TARGET_HEIGHT-1],\n",
        "    [0, TARGET_HEIGHT-1]\n",
        "])"
      ],
      "metadata": {
        "id": "0wtK-XUuGR_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Playing the video\n",
        "# display(Video(SOURCE_VIDEO, embed=True))"
      ],
      "metadata": {
        "id": "UB5LVipOD35c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For resolving perspective distortion and get the coordinates for objects as per their position in the frame\n",
        "class ViewTransformer:\n",
        "  def __init__(self, source, target):\n",
        "    source = source.astype(np.float32)\n",
        "    target = target.astype(np.float32)\n",
        "    self.matrix = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "  def transformed_points(self, points):\n",
        "    reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "    transformed_points = cv2.perspectiveTransform(reshaped_points, self.matrix)\n",
        "    return transformed_points.reshape(-1, 2)"
      ],
      "metadata": {
        "id": "3We5d2S6obDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from typing import DefaultDict\n",
        "# Getting information about the video\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = get_roboflow_model(\"yolov8x-640\")\n",
        "\n",
        "# Using byte track to track individual object to get its id using video frames\n",
        "byte_track = sv.ByteTrack(frame_rate=video_info.fps)\n",
        "\n",
        "# Getting bounding box line and text thickness\n",
        "thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\n",
        "text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "# Bounding boxes\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\n",
        "\n",
        "# Labelling the bounding box\n",
        "label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness)\n",
        "\n",
        "# Plotting the polygon box to limit the detection boundary, and calculating speed\n",
        "polygon_zone = sv.PolygonZone(SOURCE, frame_resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "# Calling the ViewTransformer\n",
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
        "\n",
        "# Getting video frames\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO)\n"
      ],
      "metadata": {
        "id": "o7qvrHGaE5KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Annotating a single frame\n",
        "frame = iter(frame_generator)\n",
        "frame = next(frame)\n",
        "\n",
        "result = model.infer(frame)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "\n",
        "# Detecting only inside polygon zone\n",
        "detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "# Labelling objects with id's\n",
        "detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "#\n",
        "points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "points = view_transformer.transformed_points(points).astype(int)\n",
        "\n",
        "# Labels list\n",
        "labels = [\n",
        "    f\"#x:{x}, y:{y}\"\n",
        "    for [x, y]\n",
        "    in points\n",
        "]\n",
        "\n",
        "# Annotating the frame\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "\n",
        "# Drawing the polygon onto the frame\n",
        "annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.RED)\n",
        "\n",
        "annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "## Looping over frames to annotate vehicles frame by frame\n",
        "# for frame in frame_generator:\n",
        "#   result = model.infer(frame)[0]\n",
        "#   detections = sv.Detections.from_inference(result)\n",
        "\n",
        "#   annotated_frame = frame.copy()\n",
        "#   annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "#   annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "\n",
        "#   # cv2_imshow(annotated_frame)\n",
        "#   if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "#     break\n",
        "\n",
        "# # Closing the window\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F5fg1f2EH5PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkCgvhBPJNRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}